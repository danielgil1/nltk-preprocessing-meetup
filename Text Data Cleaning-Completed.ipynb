{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Pre-processing text data\n",
    "\n",
    "\n",
    "Most of the text data are cleaned by following below steps.<br>\n",
    "\n",
    "- Remove punctuations and unwanted symbols\n",
    "- Tokenization - Converting a sentence into list of words\n",
    "- Remove stopwords\n",
    "- Normalization (putting everything in the same way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Orientation: Where am I?\n",
    "<img src='res/launch_1.jpg'>\n",
    "\n",
    "\n",
    "<i>Credits: Kerbal space program: Falcon 9 Space X</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is the Natural Language Toolkit?\n",
    "<img src='res/NLTK.png'>\n",
    "\n",
    "NLTK is a Python Library for working with written language data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "NLTK is free and extensively documented [here](http://www.nltk.org/).\n",
    "> Note: NLTK provides tools for tasks ranging from very simple (counting words in a text) to very complex (writing and training parsers, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From previous Introduction workshop: Read data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "text_path = 'datasets/twcs.csv'\n",
    "path=os.path.join(text_path)\n",
    "file = open(os.path.join(text_path), \"r\", encoding='UTF-8')\n",
    "text = file.read()\n",
    "lines=text.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "we can inspect the type of object we just got in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lines[1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can actually use another python library to make this easier and access only the column we need: (More about this in Pandas Introductory workshops at [Python Resplat Community](https://research.unimelb.edu.au/infrastructure/research-platform-services/training/python))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(text_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are interested the in the column \"Text\", let's put it in a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text=df['text'].tolist()\n",
    "print(type(text))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This means we have roughly 2.8M tweets to analyse!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "let's take a look at the first tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I'm going to take a protion of this text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tweets=text[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pre-processing tasks\n",
    "<img src='res/textprocessing.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can notice some special characters: @ denotes a username in twitter, we will probably need to get rid of these. We can also remove puntuaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Work with strings: Find unwanted characteres with regular expressions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Can you find a pattern??\n",
    "<img src='res/pat.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Regular expressions = Find patterns in text\n",
    "\n",
    "Look at a group of tweets and try to identify some patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# here we careate a regular expression. A word starting with @ and following next a string of letters or numbers\n",
    "regex_username=re.compile(r'@([A-Za-z0-9_]+)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Original tweet:\",tweets[0])\n",
    "print(\"Pattern found:\",re.findall(regex_username,tweets[0]))\n",
    "my_new_tweet=regex_username.sub('',tweets[0]).strip()\n",
    "print(\"Text removing the pattern:\",my_new_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src='res/regex.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's extract all usernames from tweets and put them on a list so we can use them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "users_list=list()\n",
    "for tweet in tweets:\n",
    "    users_list+=re.findall(regex_username,tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create sets for users. A set will remove the duplicates\n",
    "users=set(users_list)\n",
    "print(len(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose we want to remove all the usernames of our sample tweets just to anonymize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "anonymous_tweets=list()\n",
    "for tweet in tweets:\n",
    "    new_tweet=regex_username.sub('',tweet).strip()\n",
    "    anonymous_tweets.append(new_tweet)\n",
    "print(\"Original:\",tweets[0:10])\n",
    "print('*****')\n",
    "print(\"Anonymous\",anonymous_tweets[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Challenge: Extract all topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "regex_topics=re.compile(...) # fill in\n",
    "topic_list=list()\n",
    "for tweet in tweets:\n",
    "    topic_list+=... # fill\n",
    "    \n",
    "# print the total number of topics (remember the duplicates)\n",
    "print(..) #fill in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Work with strings: Puntuaction and symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Can I have a shortcut of symbols I can start with?\n",
    "Answer: yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use this list to go through our entire text and remove words in this list. Let's see how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text_no_punct=[]\n",
    "text='Capacity must be shown (in other work); in the law, concealment of it will do'\n",
    "for letter in text:\n",
    "    if letter not in string.punctuation:\n",
    "        text_no_punct.append(letter)\n",
    "        \n",
    "''.join(text_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Challenge: Build anonymous tweet dataset without puntuaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "no_punct_tweets=list()\n",
    "punct=string.punctuation\n",
    "for tweet in anonymous_tweets:\n",
    "    new_tweet=[]\n",
    "    for letter in tweet:\n",
    "        if letter not in punct:\n",
    "            new_tweet.append(letter)\n",
    "    no_punct_tweets.append(''.join(new_tweet))      \n",
    "    \n",
    "no_punct_tweets[100:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are interested in words and sentences. We can do this with an operation called \"tokenization\"\n",
    "\n",
    "**Tokenization = cut the text into pieces like sentences or words**\n",
    "<br><br>\n",
    "<img src=\"res/token.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can use sicssors and paper, you can cut the text by words, sentences and other combinations. NLTK has several tokenizers (including support for different languages) which you can explore [here](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize).<br>\n",
    "Let's explore the most basic ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokens=word_tokenize(tweets[5]) # try different tokenizers\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, collect all tokens into a list so we can keep use them later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "word_tokens_list=list()\n",
    "for tweet in anonymous_tweets:\n",
    "    word_tokens_list+=word_tokenize(tweet)\n",
    "\n",
    "print(len(word_tokens_list))\n",
    "word_tokens_list[100:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sentence tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sent_tokens_list=list()\n",
    "for tweet in anonymous_tweets:\n",
    "    sent_tokens_list+=sent_tokenize(tweet)\n",
    "\n",
    "print(len(sent_tokens_list))\n",
    "sent_tokens_list[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, notice you can have tokens which can be exactly the same, for example, you can see the same word with different case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenge: Try the wordpunct_tokenize and TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize,TweetTokenizer\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# this is the word punct tokenizer\n",
    "punct_tokens_list=list()\n",
    "for tweet in anonymous_tweets:\n",
    "    punct_tokens_list+=wordpunct_tokenize(tweet)\n",
    "\n",
    "print(len(punct_tokens_list))\n",
    "punct_tokens_list[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# this is a special tokenizer for tweets\n",
    "tweet_tokenize = TweetTokenizer()\n",
    "tokens=tweet_tokenize.tokenize(tweets[5]) \n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Normalization: Set the ground rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Strings have built-in functions to help us transform the text. Our goal is have the text in a normalized way so we can start analysing.\n",
    "1. Take letters to lowercase or uppercase\n",
    "2. Remove blancs\n",
    "3. Remove unwanted characters: punctuation, symbols, accents and diacritical (e.g ',`)\n",
    "5. Expand abbreviations (not covered)\n",
    "7. Remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's rembember some of the functions to work out this tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# lowercase\n",
    "word='We'\n",
    "print(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# remove spaces\n",
    "word='We '\n",
    "print(word.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# remove unwanted accents\n",
    "word='We\\'d'\n",
    "new_word=''\n",
    "for letter in word:\n",
    "    if letter not in string.punctuation:\n",
    "        new_word+=letter\n",
    "print(new_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In general we can use all string functions python has:<br>\n",
    "<img src='res/strings.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Removing stop words\n",
    "Stop words = most common words in a language that we are not interested to be part of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Challenge: Normalization\n",
    "Choose one of the token list (word/punct) and normalize every token by lowercasing, removing stop words, symbols and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lemmatization and stemming\n",
    "We have tried to normalise our text, we converted the word \"We\" to \"we\" so we don't count it only once. Now, how to handle a situation like \"child\" and \"children\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form. For instance:<br>\n",
    "am, are, is $\\Rightarrow$ be <br>\n",
    "car, cars, car's, cars' $\\Rightarrow$ car\n",
    "\n",
    "<i>(From Stanford NLP)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stemming <br>\n",
    "<img src=\"res/Stemming_Words_print.png\" width='50%'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Apply stemmer\n",
    "for word in word_tokens_list[100:150]:\n",
    "    stem_word=porter.stem(word)\n",
    "    print(word,'->',stem_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Leematisation <br>\n",
    "Grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form.<br>\n",
    "<img src='res/lemma.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "# try words like 'better' (adj), 'cats' (noun)\n",
    "print(lemmatizer.lemmatize('better',wn.ADJ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenge: Create a new list with tokens lemmatizing by verb, noun and adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hint:  check if the word changes with verb, if not try noun, and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spelling correction\n",
    "<img src='res/spelit.gif'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our goal: Identify misspelled words and try to use the correct ones. We can have several approaches, we will focus today in the most basic one, but we can try more advanced techniques in next meetups:\n",
    "1. Using regular expressions\n",
    "2. Using measures of similarity in words, language models and meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "\n",
    "for word in word_tokens_list:\n",
    "    new_word=pattern.sub(r\"\\1\\1\", word)\n",
    "    if (word!=new_word):\n",
    "        print(word,'->',new_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualization\n",
    "Remember the Frequency Distribution plot? (Introduction to NLTK Workshop), here we will use a different method to plot it, more visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word cloud\n",
    "a Word Cloud represents the importance of a token giving a bigger size to the most important ones.\n",
    "\n",
    "<img src='res/wordle_love_song.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need to use an extra library for this, called <i>wordcloud</i>.\n",
    "<br>\n",
    "To install, run this code in your command line:<br>\n",
    "`conda install -c conda-forge wordcloud`\n",
    "\n",
    "if you are using pip:<br>\n",
    "`pip install wordcloud`\n",
    "\n",
    "Now you can use the library!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We need to prepare the text first, Let's try to use the username list. We will want to put all usernames in lower case to avoid counting different words for the same username."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# set all users in one string\n",
    "flat_users=' '.join(users_list)\n",
    "\n",
    "# normalize by putting lowercase\n",
    "flat_users=flat_users.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(width=1024, height=768,max_font_size=40,collocations=False).generate(flat_users)\n",
    "plt.figure(figsize = (15, 15), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make it more pretty?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "mask = np.array(Image.open(\"res/heart.jpg\"))\n",
    "wordcloud = WordCloud(max_font_size=40,collocations=False,mask=mask).generate(flat_users)\n",
    "plt.figure(figsize = (15, 15), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenge:  Build a wordcloud \n",
    "Create two wordclouds, one with the topics and other with the whole cleaned tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bonus!!!\n",
    "Play ploting  frequency distributions using your final list of tokens!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Frequency distributions\n",
    "from nltk import Text\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "my_text=Text(..) #fill in\n",
    "fdist=FreqDist(...) #fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "More on visualization? come to the next python meetup [Python Data Viz with Matplotlib, the Godparent of Python Plotting Libraries](https://www.eventbrite.com.au/e/python-data-viz-with-matplotlib-the-godparent-of-python-plotting-libraries-tickets-48771750619)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "802.6666870117188px",
    "left": "22px",
    "top": "137.6666717529297px",
    "width": "333.9791564941406px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
